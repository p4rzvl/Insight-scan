{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!pip install -q fastapi uvicorn nest_asyncio transformers torch pillow pyngrok python-multipart bitsandbytes accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-18T16:52:19.438092Z",
     "iopub.status.busy": "2025-04-18T16:52:19.437484Z",
     "iopub.status.idle": "2025-04-18T16:52:19.709251Z",
     "shell.execute_reply": "2025-04-18T16:52:19.708588Z",
     "shell.execute_reply.started": "2025-04-18T16:52:19.438063Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Authtoken saved to configuration file: /root/.config/ngrok/ngrok.yml\n"
     ]
    }
   ],
   "source": [
    "!ngrok config add-authtoken 2uXobtEjuC5xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-18T16:52:20.809135Z",
     "iopub.status.busy": "2025-04-18T16:52:20.808520Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-18 16:52:24.844160: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1744995144.866826     118 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1744995144.873590     118 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading p4rzvl/Llama-3.2-11B-Vision-Radiology-mini on cuda…\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "605e0e32ad02473299bc74e411dfae82",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/375k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ecd0f64951d416cb45b7537ef2f6be9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 2 files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a8e43488b5149e2be113075a4392794",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00002.safetensors:   0%|          | 0.00/2.94G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d5c445a44fd41a0b878c5529d81b3cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00002.safetensors:   0%|          | 0.00/4.97G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "17f9c47d8724411ba53b12f224f0b0c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d25ab7d5fd3d4785b618b4e1c886f742",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/210 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "59c4a3c0a401438fbd4ea35e1c5da134",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "adapter_model.safetensors:   0%|          | 0.00/269M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded.\n",
      ">>> ngrok tunnel available at: https://705b-34-58-14-166.ngrok-free.app\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:     Started server process [118]\n",
      "INFO:     Waiting for application startup.\n",
      "INFO:     Application startup complete.\n",
      "INFO:     Uvicorn running on http://0.0.0.0:8000 (Press CTRL+C to quit)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:     14.139.122.100:0 - \"POST /predict HTTP/1.1\" 200 OK\n",
      "INFO:     14.139.122.100:0 - \"POST /predict HTTP/1.1\" 200 OK\n",
      "INFO:     14.139.122.100:0 - \"POST /predict HTTP/1.1\" 200 OK\n",
      "INFO:     14.139.122.100:0 - \"POST /predict HTTP/1.1\" 200 OK\n",
      "INFO:     14.139.122.100:0 - \"POST /predict HTTP/1.1\" 200 OK\n",
      "INFO:     14.139.122.100:0 - \"POST /predict HTTP/1.1\" 200 OK\n",
      "INFO:     14.139.122.100:0 - \"POST /predict HTTP/1.1\" 200 OK\n",
      "INFO:     14.139.122.100:0 - \"POST /predict HTTP/1.1\" 200 OK\n"
     ]
    }
   ],
   "source": [
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "\n",
    "import os\n",
    "import io\n",
    "import torch\n",
    "from fastapi import FastAPI, File, UploadFile, Form, HTTPException\n",
    "from fastapi.responses import JSONResponse\n",
    "from transformers import AutoProcessor, AutoModelForImageTextToText\n",
    "from PIL import Image\n",
    "from fastapi.middleware.cors import CORSMiddleware\n",
    "from pyngrok import ngrok\n",
    "import uvicorn\n",
    "\n",
    "app = FastAPI()\n",
    "app.add_middleware(\n",
    "    CORSMiddleware,\n",
    "    allow_origins=[\"*\"],  # or [\"http://localhost:5173\"]\n",
    "    allow_credentials=True,\n",
    "    allow_methods=[\"*\"],\n",
    "    allow_headers=[\"*\"],\n",
    ")\n",
    "\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "MODEL_ID = \"p4rzvl/Llama-3.2-11B-Vision-Radiology-mini\"\n",
    "\n",
    "print(f\"Loading {MODEL_ID} on {DEVICE}…\")\n",
    "processor = AutoProcessor.from_pretrained(MODEL_ID)\n",
    "model = AutoModelForImageTextToText.from_pretrained(MODEL_ID).to(DEVICE)\n",
    "print(\"Model loaded.\")\n",
    "\n",
    "@app.post(\"/predict\")\n",
    "async def predict(\n",
    "    instruction: str = Form(...),\n",
    "    file: UploadFile = File(...)\n",
    "):\n",
    "    # read image\n",
    "    try:\n",
    "        img = Image.open(io.BytesIO(await file.read())).convert(\"RGB\")\n",
    "    except Exception as e:\n",
    "        raise HTTPException(status_code=400, detail=f\"Invalid image: {e}\")\n",
    "\n",
    "    # chat template\n",
    "    messages = [\n",
    "        {\"role\": \"user\", \"content\": [\n",
    "            {\"type\": \"image\"},\n",
    "            {\"type\": \"text\",  \"text\": instruction}\n",
    "        ]}\n",
    "    ]\n",
    "    prompt = processor.apply_chat_template(messages, add_generation_prompt=True)\n",
    "\n",
    "    # tokenize multimodal inputs\n",
    "    inputs = processor(text=prompt, images=img, return_tensors=\"pt\").to(DEVICE)\n",
    "\n",
    "    # generate\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=300,\n",
    "        do_sample=True,\n",
    "        temperature=1.5,\n",
    "        top_p=0.9,\n",
    "    )\n",
    "\n",
    "    report = processor.decode(outputs[0], skip_special_tokens=True)\n",
    "    return JSONResponse({\"report\": report})\n",
    "\n",
    "\n",
    "public_url = ngrok.connect(8000, \"http\").public_url\n",
    "print(\">>> ngrok tunnel available at:\", public_url)\n",
    "\n",
    "uvicorn.run(app, host=\"0.0.0.0\", port=8000, log_level=\"info\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "# replace with the Kaggle-provided URL/port if different\n",
    "url = \"http://0.0.0.0:8000/predict\"\n",
    "files = {\"file\": open(\"path/to/img\",\"rb\")}\n",
    "data  = {\"instruction\": \"You are an expert radiographer. Describe accurately what you see in this image.\"}\n",
    "\n",
    "resp = requests.post(url, files=files, data=data)\n",
    "print(resp.json())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 3888818,
     "sourceId": 6755629,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7176529,
     "sourceId": 11453697,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31011,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
